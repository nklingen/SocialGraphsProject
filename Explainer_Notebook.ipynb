{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Muro;font-weight:bold;font-size:35px;margin:10px 0px 10px 0px;color:#ADD8E6\">Explainer notebook</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "In order to avoid a massive notebook and for organizational purposes our work is divided into different notebooks, each one focussing on a different area. For this reason, we link the relevant notebooks at every section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "\n",
    "We will focus on analyzing The Office, which is an American mockumentary sitcom television series that depicts the everyday work lives of office employees in a branch of the Dunder Mifflin Paper Company [\\[1\\]](https://en.wikipedia.org/wiki/The_Office_(American_TV_series)). We decided to focus on this because we are huge fans and it's a topic we are very familiar with.\n",
    "\n",
    "We want to a do an in-depth analysis of The Office to find some intresting insights and fun facts about the series and its characters. Given that the network is not big enough (less than 1000 characters) to find some intresting patterns in their social behaviour, our approach will heavily focus on a more qualitative analysis of the series. More specifically, we will do the following:\n",
    "\n",
    "- Model the network of characters and extract information such as who is related to who, which characters are more popular and some other basic statistics (number of links and nodes, distrbution, etc).\n",
    "- Find possible underlying communities and analyze what they have in common.\n",
    "- Extract the most descriptive words that characterize each character from their wiki pages and analyze the characters´ dialogue to explore any visible personality traits in their way of speaking, using wordclouds.\n",
    "- Determine which characters have the most lines throghout the series and see who interacts the most with whom.\n",
    "- Analyze the sentiment of the series on average throughout the seasons and episodes and also for the individual characters, to determine who are the happiest, the saddest or who are the most emotionally stable/unstable. \n",
    "- Explore the sentiment developement for the main characters throughout the series as well as determine the episodes that had a biggest emotional charge for them and why.\n",
    "- Analyze the viewership and ratings of the series per episode and find possible explanations for the observed trends, while correlating them other parameters in the network.\n",
    "- Explore the different directors for each episode, such as who had the highest ratings and who directed the most episodes.\n",
    "\n",
    "For this, we use the following datasets:\n",
    "\n",
    "- *The Office Wiki*. For retrieving the characters and the links between each other, their category, the seasons they appear in and their biographical information.\n",
    "- *The Office script*. To retrieve the dialogue of the characters that will allow us to extract their sentiment, any characterisitc expresions and the interaction network.\n",
    "- *IMBd ratings*. To explore how the ratings varied throughout the episodes.\n",
    "- *Kaggle Dataset*.  For directors and viewership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic stats\n",
    "\n",
    "To see more about the process of collecting, cleaning and pre-processing the data, please refer to the following notebooks. Here we explain how we retrieve the data from The Office Wiki through the API and how we clean the text. We discuss how we manually partition characters into communities as they are categorized on the Wiki, and format the results in a data frame with additional relevant information for the network. We also demonstrate how to retrieve links to other related characters. Finally, we compute the size of the dataset.\n",
    "\n",
    "Moreover, we download and discuss the dialogue/script and IMBd ratings datasets.\n",
    "\n",
    "[DATA RETRIEVAL AND EXPLORATION](https://github.com/nklingen/SocialGraphsProject/blob/main/Data-retrieval-and-analysis.ipynb)\n",
    "\n",
    "Lastly, we do the same for the Kaggle dataset to analyse director ratings and viewership trends.\n",
    "\n",
    "[DIRECTORS](https://github.com/nklingen/SocialGraphsProject/blob/main/Directors.ipynb)\n",
    "\n",
    "[VIEWERSHIP](https://github.com/nklingen/SocialGraphsProject/blob/main/Viewership.ipynb)\n",
    "\n",
    "From the Wiki data, we also construct and analyse the network representation of the data.\n",
    "\n",
    "[NETWORK ANALYSIS](https://github.com/nklingen/SocialGraphsProject/blob/main/Network_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, theory and analysis\n",
    "\n",
    "Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "How did you use the tools to understand your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Louvain Algorithm to detect communities in the network.\n",
    "\n",
    "[COMUNITIES](https://github.com/nklingen/SocialGraphsProject/blob/main/Comunities.ipynb)\n",
    "\n",
    "We plot and analyse the IMDb ratings by episode and average the ratings over seasons to identify outliers and detect trends.\n",
    "\n",
    "[IMDB RATINGS](https://github.com/nklingen/SocialGraphsProject/blob/main/IMDb_ratings.ipynb)\n",
    "\n",
    "We then use regular expressions and the vaderSentiment tools from the curriculum on the script to analyse character sentiment from many angles.\n",
    "\n",
    "[SENTIMENT ANALYSIS](https://github.com/nklingen/SocialGraphsProject/blob/main/Sentiment_analysis.ipynb)\n",
    "\n",
    "Lastly we use nltk and data analysis strategies from the course to create word clouds for each character, both from their WIKI pages as well as their dialogue.\n",
    "\n",
    "[WORD CLOUDS](https://github.com/nklingen/SocialGraphsProject/blob/main/Wordclouds.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion\n",
    "\n",
    "### Positive Takeaways\n",
    "In general, the insights that we gained from the raw data seemed to match our intuition and made sense with what we saw in the series. For example, the sentiment analysis and the communities were generally very accurate, and it was easy to explain the results.\n",
    "\n",
    "### Negative Takeaways\n",
    "\n",
    "The fact that we had many different data sources meant we had to spend lots of time on data cleaning and processing.\n",
    "\n",
    "We found some discrepencies in the data. For example, some finalies and special episodes had two parts that were shown one after the other. Thus, in some datasets they were numbered as one episode, while in others they were numbered as two. This led to inconsistant episode numbering. While we addressed this issue by manually verifying the graphs, with more time we would like to have been more thorough with this step so that the episode numbers lined up exactly.\n",
    "\n",
    "Lots of the data was very difficult to represent nicely, thus we also had to spend a long time making interactive plots that would illustrate the data well enough to be meaningful for analysis.\n",
    "\n",
    "The character's dialogue wasn't that insightful in terms of characterizing them, as many of the words were meaningless. Even though we tried collocations, we didn't find charactestic phrases that we would have liked to see, such as Michael's notorious \"That's what she said\". Given more time, it would be interesting to further work on this part of the analysis, to see if we can arrive at a more meaningful analysis of the dialogue.\n",
    "\n",
    "When analysing character lines per episode, we found that the dialogue dataset was not complete (some characters were missing lines from episodes they had been in). While we couldn't find a better dialogue dataset with the time provided, we believe that having acccess to complete dataset would have led to a better analysis. \n",
    "\n",
    "We had a hypothesis that Andy's screentime had a negative correlation with ratings, as many of the least popular episodes particularly revolved around his character. With more time, we would also have liked to explore this potential correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contributions\n",
    "You should write (just briefly) which group member was the main responsible for which elements of the assignment. (I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work. That’s what you should explain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
