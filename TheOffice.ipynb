{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re, urllib, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_text(name):\n",
    "\n",
    "    baseurl = \"https://theoffice.fandom.com/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = f\"titles={name}\"\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat = \"format=json\"\n",
    "\n",
    "    query = \"{0}{1}&{2}&{3}&{4}\".format(baseurl,action,title,content,dataformat)\n",
    "\n",
    "\n",
    "    with open(f'characters/{name}.txt', 'w') as f:\n",
    "\n",
    "        response = urllib.request.urlopen(query)\n",
    "        data = response.read()\n",
    "        text = data.decode('utf-8')\n",
    "        content = json.loads(text)\n",
    "        page_id = list(content['query']['pages'].keys())[0]\n",
    "        text = content['query']['pages'][page_id]['revisions'][0]['slots']['main']['*']\n",
    "        f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://theoffice.fandom.com/api.php?\"\n",
    "action = \"action=query\"\n",
    "content = \"prop=revisions&rvprop=content\"\n",
    "dataformat =\"format=json\"\n",
    "\n",
    "# characters are split across two pages\n",
    "page_titles = [\"https://theoffice.fandom.com/wiki/Category:Characters\", \"https://theoffice.fandom.com/wiki/Category:Characters?from=Merv+Bronte\"]\n",
    "categories = []\n",
    "\n",
    "\n",
    "for page_title in page_titles:\n",
    "\n",
    "    title = f\"titles={page_title}\"\n",
    "    query = \"{}{}&{}&{}&{}&rvslots=*\".format(baseurl, action, content, title, dataformat)\n",
    "\n",
    "    page = requests.get(page_title)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    character_pattern = re.compile(r'/wiki/(.*)')\n",
    "    category_pattern = re.compile(r'Category:')\n",
    "    filter_subcategories = [\"Background_Employees\", \"Clients_of_Dunder_Mifflin\", \"Main_Characters\", \"Mentioned_characters\", \"Voiced_Characters\", \"Background_Warehouse_Employees\", \"Dunder_Mifflin_family_members_and_loved_ones\", \"The_Office_Characters\", \"Angela%27s_cats\"]\n",
    "    # for each of the characters listed in the characters pages\n",
    "    for link in soup.find_all(\"a\", {\"class\": \"category-page__member-link\"}):\n",
    "\n",
    "        # capture all the href elements\n",
    "        href = link.get(\"href\")\n",
    "\n",
    "        # capture all character names in each href\n",
    "        name =  character_pattern.match(href).group(1)\n",
    "\n",
    "        # remove \"Categories\" which are classifications of characters and not characters themselves\n",
    "        if category_pattern.match(name):\n",
    "            categories.append(name)\n",
    "\n",
    "        # remove specific subcategories which are not characters themselves\n",
    "        elif name not in filter_subcategories:\n",
    "            get_character_text(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
